max_seq_len: 1024
tokenizer_name: bert-base-uncased # switch to bert tokenizer until we add [MASK] token to the llama tokenizer meta-llama/Llama-2-7b-hf
mlm_probability: 0.3

model:
  vocab_size: 100000
  hidden_size: 768
  num_hidden_layers: 22
  num_attention_heads: 12
  intermediate_size: 1152
  max_position_embeddings: 1024
  type_vocab_size: 1
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  grad_checkpointing: true

tokenizer:
  name: bert-base-uncased
  max_seq_length: ${max_seq_len}

# data_local: ./data/hebrewmodernbert/v20250428
data_local: ./data/c4
data_remote: # If blank, files must be present in data_local

train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
    predownload: 250_000
    # shuffle_block_size: 100000
  drop_last: true
  num_workers: 24

eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15 # We always evaluate at 15% masking for consistent comparison
    predownload: 250_000
  drop_last: false
  num_workers: 8

trainer:
  # max_duration: "3ep"
  max_duration: 425_000_000_000tok
  precision: amp_bf16
  # save_folder: "./checkpoints"

global_train_batch_size: 1024
device_train_microbatch_size: 64
