Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2126/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2126/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2126/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2126'

Starting training...
Model is on: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/ymt14gtw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_212654-ymt14gtw/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2134/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2134/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2134/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2134'

Starting training...
Model is on: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2142/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2142/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2142/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2142'

Starting training...
Model is on: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/ney9tzog[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_213435-ney9tzog/logs[0m
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2146/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2146/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2146/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2146'

Starting training...
Model is on: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/d62qux4e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_214618-d62qux4e/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2152/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2152/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2152/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2152'

Starting training...
Model is on: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/b1o2f20c[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_215239-b1o2f20c/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2154/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2154/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2154/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2154'

Starting training...
Model is on: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/jgn0dsbn[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_215445-jgn0dsbn/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2158/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2158/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2158/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2158'

Starting training...
Model is on: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cpu
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cpu
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cpu
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/w3wjwn85[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_215814-w3wjwn85/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2204/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2204/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2204/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2204'

Starting training...
Model is on: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2207/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2207/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2207/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2207'

Starting training...
Model is on: cuda:0
[Step 1] Optimizer state 'step' on device: cpu
[Step 1] Optimizer state 'step' moved to device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/u4zrl4u6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_220723-u4zrl4u6/logs[0m
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 2789091) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
INFO: You might want to increase the weight decay because in AdamW it is scaled by the lr. Default weight decay is ``1e-2`` -> 1e-05. Default lr is `lr=1e-3` -> 0.0008.
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: adamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2212/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2212/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2212/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2212'

Starting training...
Model is on: cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[OptimizerStateFix] Moved 'step' to cuda:0
[OptimizerStateFix] Moved 'exp_avg' to cuda:0
[OptimizerStateFix] Moved 'exp_avg_sq' to cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 1] Optimizer state 'step' on device: cuda:0
[Step 1] Optimizer state 'exp_avg' on device: cuda:0
[Step 1] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 2] Optimizer state 'step' on device: cuda:0
[Step 2] Optimizer state 'exp_avg' on device: cuda:0
[Step 2] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 3] Optimizer state 'step' on device: cuda:0
[Step 3] Optimizer state 'exp_avg' on device: cuda:0
[Step 3] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 4] Optimizer state 'step' on device: cuda:0
[Step 4] Optimizer state 'exp_avg' on device: cuda:0
[Step 4] Optimizer state 'exp_avg_sq' on device: cuda:0
[Step 5] Optimizer state 'step' on device: cuda:0
[Step 5] Optimizer state 'exp_avg' on device: cuda:0
[Step 5] Optimizer state 'exp_avg_sq' on device: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/spysh30a[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_221215-spysh30a/logs[0m
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 2797518) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2244/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2244/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2244/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2244'

Starting training...
Model is on: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/yee3jigj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_224450-yee3jigj/logs[0m
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 2822513) exited with code 130
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2245/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250507_2245/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250507_2245/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250507_2245'

Starting training...
Model is on: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/mc0g39b0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250507_224549-mc0g39b0/logs[0m
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 2826852) exited with code 143
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3218364) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=2.3326e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1102/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1102/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1102/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250508_1102'

Starting training...
Model is on: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/54z0bbvt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_110213-54z0bbvt/logs[0m
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    intermediate_size: 1152
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    intermediate_size: 1152
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1144/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1144/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1144/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250508_1144'

Starting training...
Model is on: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/ptywbrfr[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_114430-ptywbrfr/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    intermediate_size: 1152
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    intermediate_size: 1152
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1146/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1146/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1146/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250508_1146'

Starting training...
Model is on: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/pepvot2y[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_114709-pepvot2y/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3291940) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3297761) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/48259i7u[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_122342-48259i7u/logs[0m
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3304685) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3310167) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3318304) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3320906) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1239/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1239/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1239/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250508_1239'

Starting training...
Model is on: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/k2vz4wfx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_123956-k2vz4wfx/logs[0m
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3335659) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 3
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=8.9907e+07
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 3
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 4160
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1320/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1320/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1320/ckpt
n_gpus: 1
device_train_batch_size: 4160
device_eval_batch_size: 256
timestamp: '20250508_1320'

Starting training...
Model is on: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/f5mxr1j4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_132102-f5mxr1j4/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1330/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1330/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1330/ckpt
n_gpus: 1
device_train_batch_size: 520
device_eval_batch_size: 256
timestamp: '20250508_1330'

Starting training...
Model is on: cuda:0
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/lbh70wg2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_133054-lbh70wg2/logs[0m
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: sgd
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: sgd
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1351/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1351/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1351/ckpt
n_gpus: 1
device_train_batch_size: 520
device_eval_batch_size: 256
timestamp: '20250508_1351'

Starting training...
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/ljm61hjp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_135201-ljm61hjp/logs[0m
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3448202) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: sgd
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: sgd
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1352/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1352/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1352/ckpt
n_gpus: 1
device_train_batch_size: 520
device_eval_batch_size: 256
timestamp: '20250508_1352'

Starting training...
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/svdond51[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_135244-svdond51/logs[0m
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3449439) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: sgd
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: tokenizer
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: tokenizer
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: tokenizer
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: sgd
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1354/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1354/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1354/ckpt
n_gpus: 1
device_train_batch_size: 520
device_eval_batch_size: 256
timestamp: '20250508_1354'

Starting training...
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/mbq2ji32[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_135411-mbq2ji32/logs[0m
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3454025) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 1ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1358/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1358/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1358/ckpt
n_gpus: 1
device_train_batch_size: 520
device_eval_batch_size: 256
timestamp: '20250508_1358'

Starting training...
Model is on: cuda:0
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/m08kheol[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_135903-m08kheol/logs[0m
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3457968) exited with code 1
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 520
seed: 42
device_train_microbatch_size: 160
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1408/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1408/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1408/ckpt
n_gpus: 1
device_train_batch_size: 520
device_eval_batch_size: 256
timestamp: '20250508_1408'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 576
seed: 42
device_train_microbatch_size: 96
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 576
seed: 42
device_train_microbatch_size: 96
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1423/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1423/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1423/ckpt
n_gpus: 1
device_train_batch_size: 576
device_eval_batch_size: 256
timestamp: '20250508_1423'

Starting training...
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/exwgf7cy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_142336-exwgf7cy/logs[0m
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 96
seed: 42
device_train_microbatch_size: 96
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  fused: true
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 96
seed: 42
device_train_microbatch_size: 96
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: true
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1430/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1430/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1430/ckpt
n_gpus: 1
device_train_batch_size: 96
device_eval_batch_size: 256
timestamp: '20250508_1430'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 3501984) exited with code 143
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 96
seed: 42
device_train_microbatch_size: 96
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 96
seed: 42
device_train_microbatch_size: 96
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1439/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1439/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1439/ckpt
n_gpus: 1
device_train_batch_size: 96
device_eval_batch_size: 256
timestamp: '20250508_1439'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 1024
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/${run_name}/${timestamp}/composer_profiler
  torch_trace_dir: profiler/${run_name}/${timestamp}/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 1024
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
profiler:
  composer_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1445/composer_profiler
  torch_trace_dir: profiler/modern-bert-base-phase-0.1-pretrain/20250508_1445/torch_profiler
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1445/ckpt
n_gpus: 1
device_train_batch_size: 1024
device_eval_batch_size: 256
timestamp: '20250508_1445'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 1024
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 1024
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1454/ckpt
n_gpus: 1
device_train_batch_size: 1024
device_eval_batch_size: 256
timestamp: '20250508_1454'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Environment variables:
CUDA_VISIBLE_DEVICES: 6
SHELL: /bin/bash
COLORTERM: truecolor
GCC_RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib
VSCODE_DEBUGPY_ADAPTER_ENDPOINTS: /home/nlp/achimoa/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/.noConfigDebugAdapterEndpoints/endpoint-6d4da0166bffd749.txt
CONDA_BACKUP_GCC_NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-nm
HISTCONTROL: ignoredups
TERM_PROGRAM_VERSION: 3.2a
CONDA_MKL_INTERFACE_LAYER_BACKUP: 
CONDA_EXE: /home/nlp/achimoa/miniconda3/bin/conda
_CE_M: 
TMUX: /tmp/tmux-30168/default,2084769,0
build_alias: x86_64-conda-linux-gnu
CONDA_BACKUP_LDFLAGS: -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CMAKE_ARGS: -DCMAKE_AR=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ar -DCMAKE_CXX_COMPILER_AR=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar -DCMAKE_C_COMPILER_AR=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar -DCMAKE_RANLIB=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_CXX_COMPILER_RANLIB=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib -DCMAKE_C_COMPILER_RANLIB=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib -DCMAKE_LINKER=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release
HOSTNAME: hpc8h200-01
HISTSIZE: 10000
FPATH: /usr/share/lmod/lmod/init/ksh_funcs
GPROF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gprof
CONDA_BACKUP_CONDA_BUILD_SYSROOT: /home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot
CONDA_TOOLCHAIN_BUILD: x86_64-conda-linux-gnu
_CONDA_PYTHON_SYSCONFIGDATA_NAME: _sysconfigdata_x86_64_conda_cos6_linux_gnu
CONDA_BACKUP_STRIP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strip
CONDA_BACKUP_DEBUG_CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
STRINGS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strings
LMOD_SH_INIT: 1
CONDA_BACKUP_DEBUG_CPPFLAGS: -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
CONDA_BACKUP_build_alias: x86_64-conda-linux-gnu
CPP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cpp
HISTTIMEFORMAT: %h %d %H:%M:%S 
CONDA_BACKUP_DEBUG_CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
PYDEVD_DISABLE_FILE_VALIDATION: 1
CONDA_BACKUP_SIZE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-size
CONDA_BACKUP_ELFEDIT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-elfedit
CONDA_BACKUP_BUILD: x86_64-conda-linux-gnu
CONDA_BACKUP_CPP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cpp
XML_CATALOG_FILES: file:///home/nlp/achimoa/miniconda3/envs/bert24/etc/xml/catalog file:///etc/xml/catalog
LMOD_DIR: /usr/share/lmod/lmod/libexec
PWD: /home/nlp/achimoa/workspace/ModernHebrewBERT
KRB5CCNAME: KCM:
CONDA_BACKUP_CONDA_TOOLCHAIN_HOST: x86_64-conda-linux-gnu
GSETTINGS_SCHEMA_DIR: /home/nlp/achimoa/miniconda3/envs/bert24/share/glib-2.0/schemas
LOGNAME: achimoa
CONDA_ROOT: /home/nlp/achimoa/miniconda3
XDG_SESSION_TYPE: tty
CONDA_PREFIX: /home/nlp/achimoa/miniconda3/envs/bert24
MODULESHOME: /usr/share/lmod/lmod
MANPATH: /usr/share/lmod/lmod/share/man:
CONDA_BACKUP_AS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-as
CONDA_BACKUP_AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ar
GSETTINGS_SCHEMA_DIR_CONDA_BACKUP: 
CXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
BUNDLED_DEBUGPY_PATH: /home/nlp/achimoa/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy
CONDA_BACKUP_GPROF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gprof
VSCODE_GIT_ASKPASS_NODE: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/node
CONDA_TOOLCHAIN_HOST: x86_64-conda-linux-gnu
DEBUG_CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
MOTD_SHOWN: pam
LDFLAGS: -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
HOME: /home/nlp/achimoa
LANG: en_US.UTF-8
CONDA_BACKUP_GXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-g++
MESON_ARGS: -Dbuildtype=release
CONDA_BACKUP_ADDR2LINE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-addr2line
LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:
DEBUG_CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
NVCC_PREPEND_FLAGS:  -ccbin=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++ -ccbin=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
CONDA_BACKUP_OBJCOPY: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objcopy
CXX_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
SSL_CERT_DIR: /etc/pki/tls/certs
CONDA_BACKUP_CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
LMOD_SETTARG_FULL_SUPPORT: no
CONDA_BACKUP_HOST: x86_64-conda-linux-gnu
ELFEDIT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-elfedit
CONDA_PROMPT_MODIFIER: (bert24) 
GIT_ASKPASS: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/extensions/git/dist/askpass.sh
CMAKE_PREFIX_PATH: /home/nlp/achimoa/miniconda3/envs/bert24:/home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot/usr
CONDA_BACKUP_LD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld
CPPFLAGS: -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
LD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld
LMOD_VERSION: 8.7.55
CONDA_BACKUP_GCC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc
SSH_CONNECTION: 192.168.4.164 40642 192.168.4.165 22
CONDA_BACKUP_CC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
READELF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-readelf
CONDA_BACKUP_NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-nm
GXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-g++
MODULEPATH_ROOT: /usr/share/modulefiles
VSCODE_GIT_ASKPASS_EXTRA_ARGS: 
CONDA_BACKUP_LD_GOLD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld.gold
GCC_AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar
XDG_SESSION_CLASS: user
ADDR2LINE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-addr2line
LMOD_PKG: /usr/share/lmod/lmod
CONDA_BACKUP_CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CONDA_BACKUP_host_alias: x86_64-conda-linux-gnu
CONDA_BACKUP_RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ranlib
TERM: screen
_CE_CONDA: 
SIZE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-size
GCC_NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-nm
HOST: x86_64-conda-linux-gnu
CONDA_BACKUP_GCC_RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib
CONDA_BACKUP_READELF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-readelf
LESSOPEN: ||/usr/bin/lesspipe.sh %s
CC_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
USER: achimoa
CONDA_BACKUP_GCC_AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar
TMUX_PANE: %0
VSCODE_GIT_IPC_HANDLE: /run/user/30168/vscode-git-ed38aa6024.sock
CONDA_BACKUP_CXXFILT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++filt
CONDA_SHLVL: 4
AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ar
AS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-as
DEBUG_CPPFLAGS: -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
host_alias: x86_64-conda-linux-gnu
LMOD_ROOT: /usr/share/lmod
SHLVL: 4
NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-nm
BASH_ENV: /usr/share/lmod/lmod/init/bash
GCC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc
CONDA_BACKUP_CMAKE_PREFIX_PATH: /home/nlp/achimoa/miniconda3/envs/bert24:/home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot/usr
LMOD_sys: Linux
XDG_SESSION_ID: 29042
CONDA_BACKUP_CONDA_TOOLCHAIN_BUILD: x86_64-conda-linux-gnu
CONDA_BACKUP_OBJDUMP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objdump
CONDA_BACKUP_STRINGS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strings
LD_GOLD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld.gold
CONDA_PYTHON_EXE: /home/nlp/achimoa/miniconda3/bin/python
LD_LIBRARY_PATH: /usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:
CONDA_BACKUP_CC_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
XDG_RUNTIME_DIR: /run/user/30168
SSL_CERT_FILE: /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
S_COLORS: auto
PS1: (bert24) \[]633;A\][\u@\h \W]\$ \[]633;B\]
SSH_CLIENT: 192.168.4.164 40642 22
CONDA_BACKUP_CPPFLAGS: -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CONDA_DEFAULT_ENV: bert24
OBJCOPY: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objcopy
CONDA_BACKUP_CXX_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
which_declare: declare -f
VSCODE_GIT_ASKPASS_MAIN: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/extensions/git/dist/askpass-main.js
STRIP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strip
CUDA_HOME: /usr/local/cuda-12.8
NVCC_PREPEND_FLAGS_BACKUP:  -ccbin=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
XDG_DATA_DIRS: /home/nlp/achimoa/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share
OBJDUMP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objdump
BROWSER: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/helpers/browser.sh
PATH: /usr/local/cuda-12.8/bin:/home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/usr/local/cuda-12.8/bin:/home/nlp/achimoa/software/google-cloud-sdk/bin:/home/nlp/achimoa/miniconda3/envs/bert24/bin:/usr/local/cuda-12.8/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/usr/local/cuda-12.8/bin:/home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/usr/local/cuda-12.8/bin:/home/nlp/achimoa/software/google-cloud-sdk/bin:/home/nlp/achimoa/miniconda3/bin:/usr/local/cuda-12.8/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/home/nlp/achimoa/software/google-cloud-sdk/bin:/home/nlp/achimoa/miniconda3/bin:/usr/local/cuda-12.8/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin
MODULEPATH: /usr/biu/slurmdata/modulefiles:/usr/biu/slurmdata/modulefiles:/usr/biu/slurmdata/modulefiles:/etc/modulefiles:/usr/share/modulefiles:/usr/biu/slurmdata/modulefiles:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core
CC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
HISTFILESIZE: 10000
CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CXXFILT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++filt
DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/30168/bus
LMOD_CMD: /usr/share/lmod/lmod/libexec/lmod
CONDA_BACKUP_CXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
BUILD: x86_64-conda-linux-gnu
MAIL: /var/spool/mail/achimoa
CONDA_PREFIX_1: /home/nlp/achimoa/miniconda3
CONDA_PREFIX_2: /home/nlp/achimoa/miniconda3/envs/bert24
CONDA_PREFIX_3: /home/nlp/achimoa/miniconda3
RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ranlib
CONDA_BUILD_SYSROOT: /home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot
TERM_PROGRAM: tmux
MKL_INTERFACE_LAYER: LP64,GNU
VSCODE_IPC_HOOK_CLI: /run/user/30168/vscode-ipc-5ae69caa-226e-42a8-97b2-2bec175dc425.sock
BASH_FUNC_ml%%: () {  eval "$($LMOD_DIR/ml_cmd "$@")"
}
BASH_FUNC_which%%: () {  ( alias;
 eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@
}
BASH_FUNC_module%%: () {  if [ -z "${LMOD_SH_DBG_ON+x}" ]; then
 case "$-" in 
 *v*x*)
 __lmod_sh_dbg='vx'
 ;;
 *v*)
 __lmod_sh_dbg='v'
 ;;
 *x*)
 __lmod_sh_dbg='x'
 ;;
 esac;
 fi;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 set +$__lmod_sh_dbg;
 echo "Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output" 1>&2;
 fi;
 eval "$($LMOD_CMD shell "$@")" && eval "$(${LMOD_SETTARG_CMD:-:} -s sh)";
 __lmod_my_status=$?;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 echo "Shell debugging restarted" 1>&2;
 set -$__lmod_sh_dbg;
 fi;
 unset __lmod_sh_dbg;
 return $__lmod_my_status
}
_: /home/nlp/achimoa/miniconda3/envs/bert24/bin/composer
RANK: 0
WORLD_SIZE: 1
LOCAL_RANK: 0
LOCAL_WORLD_SIZE: 1
NODE_RANK: 0
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 36619
PYTHONUNBUFFERED: 1
TORCH_NCCL_ASYNC_ERROR_HANDLING: 1
CUDA_LAUNCH_BLOCKING: 1
PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:128
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 256
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 256
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1459/ckpt
n_gpus: 1
device_train_batch_size: 256
device_eval_batch_size: 256
timestamp: '20250508_1459'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mmodern-bert-base-phase-0.1-pretrain[0m at: [34mhttps://wandb.ai/asafam/HebModernBERT-phase0.1/runs/re5putzh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_145928-re5putzh/logs[0m
Environment variables:
CUDA_VISIBLE_DEVICES: 6
SHELL: /bin/bash
COLORTERM: truecolor
GCC_RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib
VSCODE_DEBUGPY_ADAPTER_ENDPOINTS: /home/nlp/achimoa/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/.noConfigDebugAdapterEndpoints/endpoint-6d4da0166bffd749.txt
CONDA_BACKUP_GCC_NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-nm
HISTCONTROL: ignoredups
TERM_PROGRAM_VERSION: 3.2a
CONDA_MKL_INTERFACE_LAYER_BACKUP: 
CONDA_EXE: /home/nlp/achimoa/miniconda3/bin/conda
_CE_M: 
TMUX: /tmp/tmux-30168/default,2084769,0
build_alias: x86_64-conda-linux-gnu
CONDA_BACKUP_LDFLAGS: -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CMAKE_ARGS: -DCMAKE_AR=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ar -DCMAKE_CXX_COMPILER_AR=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar -DCMAKE_C_COMPILER_AR=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar -DCMAKE_RANLIB=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_CXX_COMPILER_RANLIB=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib -DCMAKE_C_COMPILER_RANLIB=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib -DCMAKE_LINKER=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release
HOSTNAME: hpc8h200-01
HISTSIZE: 10000
FPATH: /usr/share/lmod/lmod/init/ksh_funcs
GPROF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gprof
CONDA_BACKUP_CONDA_BUILD_SYSROOT: /home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot
CONDA_TOOLCHAIN_BUILD: x86_64-conda-linux-gnu
_CONDA_PYTHON_SYSCONFIGDATA_NAME: _sysconfigdata_x86_64_conda_cos6_linux_gnu
CONDA_BACKUP_STRIP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strip
CONDA_BACKUP_DEBUG_CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
STRINGS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strings
LMOD_SH_INIT: 1
CONDA_BACKUP_DEBUG_CPPFLAGS: -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
CONDA_BACKUP_build_alias: x86_64-conda-linux-gnu
CPP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cpp
HISTTIMEFORMAT: %h %d %H:%M:%S 
CONDA_BACKUP_DEBUG_CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
PYDEVD_DISABLE_FILE_VALIDATION: 1
CONDA_BACKUP_SIZE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-size
CONDA_BACKUP_ELFEDIT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-elfedit
CONDA_BACKUP_BUILD: x86_64-conda-linux-gnu
CONDA_BACKUP_CPP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cpp
XML_CATALOG_FILES: file:///home/nlp/achimoa/miniconda3/envs/bert24/etc/xml/catalog file:///etc/xml/catalog
LMOD_DIR: /usr/share/lmod/lmod/libexec
PWD: /home/nlp/achimoa/workspace/ModernHebrewBERT
KRB5CCNAME: KCM:
CONDA_BACKUP_CONDA_TOOLCHAIN_HOST: x86_64-conda-linux-gnu
GSETTINGS_SCHEMA_DIR: /home/nlp/achimoa/miniconda3/envs/bert24/share/glib-2.0/schemas
LOGNAME: achimoa
CONDA_ROOT: /home/nlp/achimoa/miniconda3
XDG_SESSION_TYPE: tty
CONDA_PREFIX: /home/nlp/achimoa/miniconda3/envs/bert24
MODULESHOME: /usr/share/lmod/lmod
MANPATH: /usr/share/lmod/lmod/share/man:
CONDA_BACKUP_AS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-as
CONDA_BACKUP_AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ar
GSETTINGS_SCHEMA_DIR_CONDA_BACKUP: 
CXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
BUNDLED_DEBUGPY_PATH: /home/nlp/achimoa/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy
CONDA_BACKUP_GPROF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gprof
VSCODE_GIT_ASKPASS_NODE: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/node
CONDA_TOOLCHAIN_HOST: x86_64-conda-linux-gnu
DEBUG_CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
MOTD_SHOWN: pam
LDFLAGS: -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/nlp/achimoa/miniconda3/envs/bert24/lib -Wl,-rpath-link,/home/nlp/achimoa/miniconda3/envs/bert24/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/lib  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
HOME: /home/nlp/achimoa
LANG: en_US.UTF-8
CONDA_BACKUP_GXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-g++
MESON_ARGS: -Dbuildtype=release
CONDA_BACKUP_ADDR2LINE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-addr2line
LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:
DEBUG_CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
NVCC_PREPEND_FLAGS:  -ccbin=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++ -ccbin=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
CONDA_BACKUP_OBJCOPY: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objcopy
CXX_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
SSL_CERT_DIR: /etc/pki/tls/certs
CONDA_BACKUP_CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
LMOD_SETTARG_FULL_SUPPORT: no
CONDA_BACKUP_HOST: x86_64-conda-linux-gnu
ELFEDIT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-elfedit
CONDA_PROMPT_MODIFIER: (bert24) 
GIT_ASKPASS: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/extensions/git/dist/askpass.sh
CMAKE_PREFIX_PATH: /home/nlp/achimoa/miniconda3/envs/bert24:/home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot/usr
CONDA_BACKUP_LD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld
CPPFLAGS: -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
LD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld
LMOD_VERSION: 8.7.55
CONDA_BACKUP_GCC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc
SSH_CONNECTION: 192.168.4.164 40642 192.168.4.165 22
CONDA_BACKUP_CC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
READELF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-readelf
CONDA_BACKUP_NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-nm
GXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-g++
MODULEPATH_ROOT: /usr/share/modulefiles
VSCODE_GIT_ASKPASS_EXTRA_ARGS: 
CONDA_BACKUP_LD_GOLD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld.gold
GCC_AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar
XDG_SESSION_CLASS: user
ADDR2LINE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-addr2line
LMOD_PKG: /usr/share/lmod/lmod
CONDA_BACKUP_CXXFLAGS: -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CONDA_BACKUP_host_alias: x86_64-conda-linux-gnu
CONDA_BACKUP_RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ranlib
TERM: screen
_CE_CONDA: 
SIZE: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-size
GCC_NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-nm
HOST: x86_64-conda-linux-gnu
CONDA_BACKUP_GCC_RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ranlib
CONDA_BACKUP_READELF: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-readelf
LESSOPEN: ||/usr/bin/lesspipe.sh %s
CC_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
USER: achimoa
CONDA_BACKUP_GCC_AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc-ar
TMUX_PANE: %0
VSCODE_GIT_IPC_HANDLE: /run/user/30168/vscode-git-ed38aa6024.sock
CONDA_BACKUP_CXXFILT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++filt
CONDA_SHLVL: 4
AR: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ar
AS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-as
DEBUG_CPPFLAGS: -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include
host_alias: x86_64-conda-linux-gnu
LMOD_ROOT: /usr/share/lmod
SHLVL: 4
NM: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-nm
BASH_ENV: /usr/share/lmod/lmod/init/bash
GCC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-gcc
CONDA_BACKUP_CMAKE_PREFIX_PATH: /home/nlp/achimoa/miniconda3/envs/bert24:/home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot/usr
LMOD_sys: Linux
XDG_SESSION_ID: 29042
CONDA_BACKUP_CONDA_TOOLCHAIN_BUILD: x86_64-conda-linux-gnu
CONDA_BACKUP_OBJDUMP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objdump
CONDA_BACKUP_STRINGS: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strings
LD_GOLD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ld.gold
CONDA_PYTHON_EXE: /home/nlp/achimoa/miniconda3/bin/python
LD_LIBRARY_PATH: /usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:
CONDA_BACKUP_CC_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
XDG_RUNTIME_DIR: /run/user/30168
SSL_CERT_FILE: /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
S_COLORS: auto
PS1: (bert24) \[]633;A\][\u@\h \W]\$ \[]633;B\]
SSH_CLIENT: 192.168.4.164 40642 22
CONDA_BACKUP_CPPFLAGS: -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CONDA_DEFAULT_ENV: bert24
OBJCOPY: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objcopy
CONDA_BACKUP_CXX_FOR_BUILD: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
which_declare: declare -f
VSCODE_GIT_ASKPASS_MAIN: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/extensions/git/dist/askpass-main.js
STRIP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-strip
CUDA_HOME: /usr/local/cuda-12.8
NVCC_PREPEND_FLAGS_BACKUP:  -ccbin=/home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
XDG_DATA_DIRS: /home/nlp/achimoa/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share
OBJDUMP: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-objdump
BROWSER: /home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/helpers/browser.sh
PATH: /usr/local/cuda-12.8/bin:/home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/usr/local/cuda-12.8/bin:/home/nlp/achimoa/software/google-cloud-sdk/bin:/home/nlp/achimoa/miniconda3/envs/bert24/bin:/usr/local/cuda-12.8/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/usr/local/cuda-12.8/bin:/home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/usr/local/cuda-12.8/bin:/home/nlp/achimoa/software/google-cloud-sdk/bin:/home/nlp/achimoa/miniconda3/bin:/usr/local/cuda-12.8/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.vscode-server/cli/servers/Stable-17baf841131aa23349f217ca7c570c76ee87b957/server/bin/remote-cli:/home/nlp/achimoa/software/google-cloud-sdk/bin:/home/nlp/achimoa/miniconda3/bin:/usr/local/cuda-12.8/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin:/home/nlp/achimoa/.local/bin:/home/nlp/achimoa/bin
MODULEPATH: /usr/biu/slurmdata/modulefiles:/usr/biu/slurmdata/modulefiles:/usr/biu/slurmdata/modulefiles:/etc/modulefiles:/usr/share/modulefiles:/usr/biu/slurmdata/modulefiles:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core
CC: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-cc
HISTFILESIZE: 10000
CFLAGS: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/nlp/achimoa/miniconda3/envs/bert24/include  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs  -I/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/include  -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib -L/home/nlp/achimoa/miniconda3/envs/bert24/targets/x86_64-linux/lib/stubs
CXXFILT: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++filt
DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/30168/bus
LMOD_CMD: /usr/share/lmod/lmod/libexec/lmod
CONDA_BACKUP_CXX: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-c++
BUILD: x86_64-conda-linux-gnu
MAIL: /var/spool/mail/achimoa
CONDA_PREFIX_1: /home/nlp/achimoa/miniconda3
CONDA_PREFIX_2: /home/nlp/achimoa/miniconda3/envs/bert24
CONDA_PREFIX_3: /home/nlp/achimoa/miniconda3
RANLIB: /home/nlp/achimoa/miniconda3/envs/bert24/bin/x86_64-conda-linux-gnu-ranlib
CONDA_BUILD_SYSROOT: /home/nlp/achimoa/miniconda3/envs/bert24/x86_64-conda-linux-gnu/sysroot
TERM_PROGRAM: tmux
MKL_INTERFACE_LAYER: LP64,GNU
VSCODE_IPC_HOOK_CLI: /run/user/30168/vscode-ipc-5ae69caa-226e-42a8-97b2-2bec175dc425.sock
BASH_FUNC_ml%%: () {  eval "$($LMOD_DIR/ml_cmd "$@")"
}
BASH_FUNC_which%%: () {  ( alias;
 eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@
}
BASH_FUNC_module%%: () {  if [ -z "${LMOD_SH_DBG_ON+x}" ]; then
 case "$-" in 
 *v*x*)
 __lmod_sh_dbg='vx'
 ;;
 *v*)
 __lmod_sh_dbg='v'
 ;;
 *x*)
 __lmod_sh_dbg='x'
 ;;
 esac;
 fi;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 set +$__lmod_sh_dbg;
 echo "Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output" 1>&2;
 fi;
 eval "$($LMOD_CMD shell "$@")" && eval "$(${LMOD_SETTARG_CMD:-:} -s sh)";
 __lmod_my_status=$?;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 echo "Shell debugging restarted" 1>&2;
 set -$__lmod_sh_dbg;
 fi;
 unset __lmod_sh_dbg;
 return $__lmod_my_status
}
_: /home/nlp/achimoa/miniconda3/envs/bert24/bin/composer
RANK: 0
WORLD_SIZE: 1
LOCAL_RANK: 0
LOCAL_WORLD_SIZE: 1
NODE_RANK: 0
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 40345
PYTHONUNBUFFERED: 1
TORCH_NCCL_ASYNC_ERROR_HANDLING: 1
CUDA_LAUNCH_BLOCKING: 1
PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:128
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 512
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 64
seed: 42
device_train_microbatch_size: 16
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 512
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 512
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 512
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 64
seed: 42
device_train_microbatch_size: 16
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1507/ckpt
n_gpus: 1
device_train_batch_size: 64
device_eval_batch_size: 256
timestamp: '20250508_1507'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 512
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 64
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
device: gpu
max_seq_len: 512
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 512
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 512
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 64
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1512/ckpt
n_gpus: 1
device_train_batch_size: 64
device_eval_batch_size: 256
timestamp: '20250508_1512'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 512
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 64
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 512
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 512
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 512
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 64
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1519/ckpt
n_gpus: 1
device_train_batch_size: 64
device_eval_batch_size: 256
timestamp: '20250508_1519'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 256
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 256
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1521/ckpt
n_gpus: 1
device_train_batch_size: 256
device_eval_batch_size: 256
timestamp: '20250508_1521'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 12
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 256
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.2708e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 12
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 256
seed: 42
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1523/ckpt
n_gpus: 1
device_train_batch_size: 256
device_eval_batch_size: 256
timestamp: '20250508_1523'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Training using config: 
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: ${max_seq_len}
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    mlm_probability: ${mlm_probability}
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ${data_local}
    remote: ${data_remote}
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 386
seed: 17
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/${run_name}/${timestamp}/ckpt

Initializing model...
n_params=1.6838e+08
Building train loader...
Building eval loader...
Logging config...
model:
  name: flex_bert
  model_config:
    normalization: layernorm
    hidden_act: gelu
    num_attention_heads: 12
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    layer_norm_eps: 1.0e-05
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: false
    final_norm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    max_position_embeddings: 1024
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: mlp
    mlp_out_bias: false
    norm_kwargs:
      eps: 1.0e-06
      bias: false
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base_local: 10000.0
    rotary_emb_base: 160000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    init_method: full_megatron
    init_std: 0.02
    init_cutoff_factor: 2.0
    init_small_embedding: false
    deterministic_fa2: false
    initial_attention_layer: null
    initial_bert_layer: null
    initial_mlp_layer: null
    num_initial_layers: 0
    skip_first_prenorm: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    pad_logits: false
    compile_model: false
    vocab_size: 100000
  recompute_metric_loss: false
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
data_local: ./data/hebrewmodernbert/v20250428
data_remote: null
max_seq_len: 1024
tokenizer_name: bert-base-uncased
mlm_probability: 0.3
run_name: modern-bert-base-phase-0.1-pretrain
train_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: train
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./data/hebrewmodernbert/v20250428
    remote: null
    split: validation
    tokenizer_name: bert-base-uncased
    max_seq_len: 1024
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 8
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.02
optimizer:
  name: decoupled_stableadamw
  lr: 0.0008
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-05
  filter_bias_norm_wd: true
max_duration: 9ep
eval_interval: 10000ba
global_train_batch_size: 386
seed: 17
device_train_microbatch_size: 64
precision: amp_bf16
global_eval_batch_size: 256
device_eval_microbatch_size: 128
progress_bar: true
log_to_console: false
console_log_interval: 10ba
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
callbacks:
  speed_monitor:
    window_size: 50
    gpu_flops_available:
      0: 130000000000000.0
loggers:
  wandb:
    project: HebModernBERT-phase0.1
    entity: asafam
save_interval: 3500ba
save_num_checkpoints_to_keep: 10
save_folder: checkpoints/modern-bert-base-phase-0.1-pretrain/20250508_1528/ckpt
n_gpus: 1
device_train_batch_size: 386
device_eval_batch_size: 256
timestamp: '20250508_1528'

Starting training...
Killing training processes
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
